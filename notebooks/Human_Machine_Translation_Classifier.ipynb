{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mQDt73qwWNp1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import string\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from nltk.translate import nist_score\n",
    "from nltk.translate import gleu_score\n",
    "from nltk.translate import ribes_score\n",
    "from nltk.translate import chrf_score\n",
    "\n",
    "import similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(current_file: str):\n",
    "    text_data = list()\n",
    "    if os.path.exists(current_file):\n",
    "        open_file = open(current_file, 'r', encoding=\"utf-8\")\n",
    "        f = open_file.read().split('\\n')  \n",
    "        list_data = [f[x:x+5] for x in range(0, len(f), 6)]\n",
    "        df = pd.DataFrame(list_data, columns =['Source', 'Reference', \"Candidate\", \"Bleu_Score\", \"Label\" ]) \n",
    "        df[\"Bleu_Score\"] = df[\"Bleu_Score\"].apply(pd.to_numeric)\n",
    "\n",
    "    else: \"Error: file does not exist\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_nist(row):\n",
    "    return nist_score.corpus_nist([[row[\"Reference\"].split(\" \")]], [row[\"Candidate\"].split(\" \")])\n",
    "\n",
    "def calc_gleu(row):\n",
    "    return gleu_score.corpus_gleu([[row[\"Reference\"].split(\" \")]], [row[\"Candidate\"].split(\" \")])\n",
    "\n",
    "def calc_ribes(row):\n",
    "    return ribes_score.corpus_ribes([[row[\"Reference\"].split(\" \")]], [row[\"Candidate\"].split(\" \")])\n",
    "\n",
    "def calc_chrf(row):\n",
    "    return chrf_score.corpus_chrf([row[\"Reference\"].split(\" \")], [row[\"Candidate\"].split(\" \")])\n",
    "\n",
    "def calc_avmax(row):\n",
    "    model = similarity_score.load_model()\n",
    "    score = similarity_score.score(row[\"Reference\"],row[\"Candidate\"],model)\n",
    "    \n",
    "# def calc_wer(row, print_matrix=False):\n",
    "#     hyp = row[\"Candidate\"]\n",
    "#     ref = row[\"Reference\"]\n",
    "#     N = len(hyp)\n",
    "#     M = len(ref)\n",
    "#     L = np.zeros((N,M))\n",
    "#     for i in range(0, N):\n",
    "#         for j in range(0, M):\n",
    "#             if min(i,j) == 0:\n",
    "#                 L[i,j] = max(i,j)\n",
    "#             else:\n",
    "#                 deletion = L[i-1,j] + 1\n",
    "#                 insertion = L[i,j-1] + 1\n",
    "#                 sub = 1 if hyp[i] != ref[j] else 0\n",
    "#                 substitution = L[i-1,j-1] + sub\n",
    "#                 L[i,j] = min(deletion, min(insertion, substitution))\n",
    "#                 # print(\"{} - {}: del {} ins {} sub {} s {}\".format(hyp[i], ref[j], deletion, insertion, substitution, sub))\n",
    "#     if print_matrix:\n",
    "#         print(\"WER matrix ({}x{}): \".format(N, M))\n",
    "#         print(L)\n",
    "#     return int(L[N-1, M-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df[\"Ref_Length\"] = df[\"Reference\"].apply(lambda x: len(x))\n",
    "    df[\"Can_Length\"] = df[\"Candidate\"].apply(lambda x: len(x))\n",
    "    df[\"Src_Length\"] = df[\"Source\"].apply(lambda x: len(x))\n",
    "    df[\"target\"] = df[\"Label\"].apply(lambda x: 1 if x == \"H\" else 0)\n",
    "    \n",
    "    df[\"NIST_Score\"] = df.apply(lambda row: calc_nist(row), axis=1)\n",
    "    df[\"Gleu_Score\"] = df.apply(lambda row: calc_gleu(row), axis=1)\n",
    "    #df[\"Ribes_Score\"] = df.apply(lambda row: calc_ribes(row), axis=1)\n",
    "    df[\"Chrf_Score\"] = df.apply(lambda row: calc_chrf(row), axis=1)\n",
    "    #df[\"AvMax_Score\"] = df.apply(lambda row: calc_avmax(row), axis=1)\n",
    "#     df[\"Wer_Score\"] = df.apply(lambda row: calc_wer(row), axis=1)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data(\"/Users/anthonycuturrufo/Documents/School/Artzi_Eval/train.txt\")\n",
    "dataset = add_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Candidate</th>\n",
       "      <th>Bleu_Score</th>\n",
       "      <th>Label</th>\n",
       "      <th>Ref_Length</th>\n",
       "      <th>Can_Length</th>\n",
       "      <th>Src_Length</th>\n",
       "      <th>target</th>\n",
       "      <th>NIST_Score</th>\n",
       "      <th>Gleu_Score</th>\n",
       "      <th>Chrf_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>巴林 公主 下 嫁 美 大兵 惊 世 婚姻 五 年 宣告 破裂</td>\n",
       "      <td>bahraini princess marries us soldier , astonis...</td>\n",
       "      <td>bahraini princess marries a u.s. soldier ; ast...</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>H</td>\n",
       "      <td>75</td>\n",
       "      <td>83</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1.423246</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.539032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>巴林 公主 梅 丽 安 ・ 哈 里 发 下 嫁 美国 陆 战 队 大兵 强 生 , 曾 获 ...</td>\n",
       "      <td>the star-crossed marriage between bahraini pri...</td>\n",
       "      <td>u.s. television stations had once feted the ma...</td>\n",
       "      <td>0.6531</td>\n",
       "      <td>H</td>\n",
       "      <td>234</td>\n",
       "      <td>251</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>3.941611</td>\n",
       "      <td>0.427711</td>\n",
       "      <td>0.700914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>梅 丽 安 是 海湾 国家 巴林 王室 的 成员 , 强 生 冒 着 赔 上 军旅 生涯 的...</td>\n",
       "      <td>meriam is a member of the gulf country bahrain...</td>\n",
       "      <td>meri gulf state of bahrain , the royal family ...</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>M</td>\n",
       "      <td>198</td>\n",
       "      <td>183</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>1.663802</td>\n",
       "      <td>0.102740</td>\n",
       "      <td>0.350299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>不过 , 据 拉 斯 维 加 斯 评论 报 报导 , 才 过 了 五 年 , 两 人 就 对...</td>\n",
       "      <td>but the las vegas review-journal reported that...</td>\n",
       "      <td>however , according to the las vegas , comment...</td>\n",
       "      <td>0.3646</td>\n",
       "      <td>M</td>\n",
       "      <td>220</td>\n",
       "      <td>174</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>1.998639</td>\n",
       "      <td>0.171233</td>\n",
       "      <td>0.361169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>他们 两 人 在 一九九九年 相遇 , 当时 强 生 还是 职业 军人 , 派 驻 在 巴林 .</td>\n",
       "      <td>the pair met in 1999 when career military man ...</td>\n",
       "      <td>the two met in 1999 , when johnson was still a...</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>H</td>\n",
       "      <td>80</td>\n",
       "      <td>89</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>3.045229</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.782057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Source  \\\n",
       "0                    巴林 公主 下 嫁 美 大兵 惊 世 婚姻 五 年 宣告 破裂   \n",
       "1  巴林 公主 梅 丽 安 ・ 哈 里 发 下 嫁 美国 陆 战 队 大兵 强 生 , 曾 获 ...   \n",
       "2  梅 丽 安 是 海湾 国家 巴林 王室 的 成员 , 强 生 冒 着 赔 上 军旅 生涯 的...   \n",
       "3  不过 , 据 拉 斯 维 加 斯 评论 报 报导 , 才 过 了 五 年 , 两 人 就 对...   \n",
       "4   他们 两 人 在 一九九九年 相遇 , 当时 强 生 还是 职业 军人 , 派 驻 在 巴林 .   \n",
       "\n",
       "                                           Reference  \\\n",
       "0  bahraini princess marries us soldier , astonis...   \n",
       "1  the star-crossed marriage between bahraini pri...   \n",
       "2  meriam is a member of the gulf country bahrain...   \n",
       "3  but the las vegas review-journal reported that...   \n",
       "4  the pair met in 1999 when career military man ...   \n",
       "\n",
       "                                           Candidate  Bleu_Score Label  \\\n",
       "0  bahraini princess marries a u.s. soldier ; ast...      0.3125     H   \n",
       "1  u.s. television stations had once feted the ma...      0.6531     H   \n",
       "2  meri gulf state of bahrain , the royal family ...      0.3784     M   \n",
       "3  however , according to the las vegas , comment...      0.3646     M   \n",
       "4  the two met in 1999 , when johnson was still a...      0.7778     H   \n",
       "\n",
       "   Ref_Length  Can_Length  Src_Length  target  NIST_Score  Gleu_Score  \\\n",
       "0          75          83          31       1    1.423246    0.173913   \n",
       "1         234         251         129       1    3.941611    0.427711   \n",
       "2         198         183         111       0    1.663802    0.102740   \n",
       "3         220         174         106       0    1.998639    0.171233   \n",
       "4          80          89          48       1    3.045229    0.409091   \n",
       "\n",
       "   Chrf_Score  \n",
       "0    0.539032  \n",
       "1    0.700914  \n",
       "2    0.350299  \n",
       "3    0.361169  \n",
       "4    0.782057  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['Chrf_Score','Gleu_Score','Bleu_Score','Ref_Length','Can_Length','Src_Length']\n",
    "output_label = ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = np.stack([dataset[col].values for col in numerical_columns], 1)\n",
    "numerical_data = torch.tensor(numerical_data, dtype=torch.float)\n",
    "outputs = torch.tensor(dataset[output_label].values).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records = 584\n",
    "test_records = int(total_records * .15)\n",
    "\n",
    "numerical_train_data = numerical_data[:total_records-test_records]\n",
    "numerical_test_data = numerical_data[total_records-test_records:total_records]\n",
    "train_outputs = outputs[:total_records-test_records]\n",
    "test_outputs = outputs[total_records-test_records:total_records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "        input_size = num_numerical_cols\n",
    "        all_layers = []\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "        \n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "        \n",
    "    def forward(self, x_numerical):     \n",
    "        x_numerical = self.batch_norm_num(x_numerical)\n",
    "        x = x_numerical\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 0.74490833\n",
      "epoch:  26 loss: 0.72360450\n",
      "epoch:  51 loss: 0.67457569\n",
      "epoch:  76 loss: 0.65862125\n",
      "epoch: 101 loss: 0.63180447\n",
      "epoch: 126 loss: 0.62117970\n",
      "epoch: 151 loss: 0.58153731\n",
      "epoch: 176 loss: 0.60210782\n",
      "epoch: 201 loss: 0.57821691\n",
      "epoch: 226 loss: 0.56562877\n",
      "epoch: 251 loss: 0.56600058\n",
      "epoch: 276 loss: 0.55627966\n",
      "epoch: 301 loss: 0.55889189\n",
      "epoch: 326 loss: 0.53891736\n",
      "epoch: 351 loss: 0.54520136\n",
      "epoch: 376 loss: 0.51729578\n",
      "epoch: 401 loss: 0.53218031\n",
      "epoch: 426 loss: 0.52084929\n",
      "epoch: 451 loss: 0.52865058\n",
      "epoch: 476 loss: 0.49797702\n",
      "epoch: 501 loss: 0.50828910\n",
      "epoch: 526 loss: 0.51344401\n",
      "epoch: 551 loss: 0.49943775\n",
      "epoch: 576 loss: 0.51492643\n",
      "epoch: 601 loss: 0.47842064\n",
      "epoch: 626 loss: 0.50445056\n",
      "epoch: 651 loss: 0.49704301\n",
      "epoch: 676 loss: 0.50963145\n",
      "epoch: 701 loss: 0.48419082\n",
      "epoch: 726 loss: 0.48984423\n",
      "epoch: 751 loss: 0.50243688\n",
      "epoch: 776 loss: 0.49096698\n",
      "epoch: 801 loss: 0.50001919\n",
      "epoch: 826 loss: 0.47625959\n",
      "epoch: 851 loss: 0.49316213\n",
      "epoch: 876 loss: 0.48709309\n",
      "epoch: 901 loss: 0.49054176\n",
      "epoch: 926 loss: 0.50147104\n",
      "epoch: 951 loss: 0.50151765\n",
      "epoch: 976 loss: 0.48878488\n",
      "epoch: 1001 loss: 0.47982293\n",
      "epoch: 1026 loss: 0.48076996\n",
      "epoch: 1051 loss: 0.48376375\n",
      "epoch: 1076 loss: 0.48719391\n",
      "epoch: 1101 loss: 0.48749265\n",
      "epoch: 1126 loss: 0.48547870\n",
      "epoch: 1151 loss: 0.46196675\n",
      "epoch: 1176 loss: 0.48502031\n",
      "epoch: 1201 loss: 0.46689877\n",
      "epoch: 1226 loss: 0.47916245\n",
      "epoch: 1251 loss: 0.50755042\n",
      "epoch: 1276 loss: 0.48028842\n",
      "epoch: 1301 loss: 0.46840885\n",
      "epoch: 1326 loss: 0.46248430\n",
      "epoch: 1351 loss: 0.46760315\n",
      "epoch: 1376 loss: 0.47799236\n",
      "epoch: 1401 loss: 0.48055962\n",
      "epoch: 1426 loss: 0.48261032\n",
      "epoch: 1451 loss: 0.46333832\n",
      "epoch: 1476 loss: 0.47842896\n",
      "epoch: 1501 loss: 0.47048178\n",
      "epoch: 1526 loss: 0.49213168\n",
      "epoch: 1551 loss: 0.46262705\n",
      "epoch: 1576 loss: 0.45853364\n",
      "epoch: 1601 loss: 0.46842232\n",
      "epoch: 1626 loss: 0.46441624\n",
      "epoch: 1651 loss: 0.47473752\n",
      "epoch: 1676 loss: 0.46950990\n",
      "epoch: 1701 loss: 0.47277322\n",
      "epoch: 1726 loss: 0.47753379\n",
      "epoch: 1751 loss: 0.47271419\n",
      "epoch: 1776 loss: 0.46209633\n",
      "epoch: 1801 loss: 0.45030335\n",
      "epoch: 1826 loss: 0.45514515\n",
      "epoch: 1851 loss: 0.46930209\n",
      "epoch: 1876 loss: 0.45989993\n",
      "epoch: 1901 loss: 0.47014457\n",
      "epoch: 1926 loss: 0.45074993\n",
      "epoch: 1951 loss: 0.45427170\n",
      "epoch: 1976 loss: 0.47021079\n",
      "epoch: 2001 loss: 0.46736333\n",
      "epoch: 2026 loss: 0.46541592\n",
      "epoch: 2051 loss: 0.47198454\n",
      "epoch: 2076 loss: 0.46556994\n",
      "epoch: 2101 loss: 0.45656502\n",
      "epoch: 2126 loss: 0.46272582\n",
      "epoch: 2151 loss: 0.46345568\n",
      "epoch: 2176 loss: 0.47899994\n",
      "epoch: 2201 loss: 0.44826272\n",
      "epoch: 2226 loss: 0.44010711\n",
      "epoch: 2251 loss: 0.45882392\n",
      "epoch: 2276 loss: 0.46258062\n",
      "epoch: 2301 loss: 0.46022043\n",
      "epoch: 2326 loss: 0.46323186\n",
      "epoch: 2351 loss: 0.47599244\n",
      "epoch: 2376 loss: 0.44590765\n",
      "epoch: 2401 loss: 0.46340069\n",
      "epoch: 2426 loss: 0.45039260\n",
      "epoch: 2451 loss: 0.44895533\n",
      "epoch: 2476 loss: 0.44725809\n",
      "epoch: 2501 loss: 0.44381112\n",
      "epoch: 2526 loss: 0.45454854\n",
      "epoch: 2551 loss: 0.45939696\n",
      "epoch: 2576 loss: 0.45207316\n",
      "epoch: 2601 loss: 0.44228503\n",
      "epoch: 2626 loss: 0.47025448\n",
      "epoch: 2651 loss: 0.45698446\n",
      "epoch: 2676 loss: 0.43828762\n",
      "epoch: 2701 loss: 0.44234776\n",
      "epoch: 2726 loss: 0.45606330\n",
      "epoch: 2751 loss: 0.44423005\n",
      "epoch: 2776 loss: 0.45703781\n",
      "epoch: 2801 loss: 0.44112423\n",
      "epoch: 2826 loss: 0.44239041\n",
      "epoch: 2851 loss: 0.47633323\n",
      "epoch: 2876 loss: 0.45258796\n",
      "epoch: 2901 loss: 0.44197175\n",
      "epoch: 2926 loss: 0.45851025\n",
      "epoch: 2951 loss: 0.46698728\n",
      "epoch: 2976 loss: 0.44264695\n",
      "epoch: 3001 loss: 0.45029163\n",
      "epoch: 3026 loss: 0.45305625\n",
      "epoch: 3051 loss: 0.45761943\n",
      "epoch: 3076 loss: 0.45594114\n",
      "epoch: 3101 loss: 0.45907277\n",
      "epoch: 3126 loss: 0.44893616\n",
      "epoch: 3151 loss: 0.46185488\n",
      "epoch: 3176 loss: 0.44964683\n",
      "epoch: 3201 loss: 0.42346808\n",
      "epoch: 3226 loss: 0.43060058\n",
      "epoch: 3251 loss: 0.43407318\n",
      "epoch: 3276 loss: 0.44931027\n",
      "epoch: 3301 loss: 0.44992134\n",
      "epoch: 3326 loss: 0.44866630\n",
      "epoch: 3351 loss: 0.44632083\n",
      "epoch: 3376 loss: 0.43659747\n",
      "epoch: 3401 loss: 0.43590802\n",
      "epoch: 3426 loss: 0.45275471\n",
      "epoch: 3451 loss: 0.43991476\n",
      "epoch: 3476 loss: 0.45932585\n",
      "epoch: 3501 loss: 0.44536707\n",
      "epoch: 3526 loss: 0.42432702\n",
      "epoch: 3551 loss: 0.40991953\n",
      "epoch: 3576 loss: 0.43416309\n",
      "epoch: 3601 loss: 0.43289632\n",
      "epoch: 3626 loss: 0.44316974\n",
      "epoch: 3651 loss: 0.44203225\n",
      "epoch: 3676 loss: 0.44248375\n",
      "epoch: 3701 loss: 0.41950333\n",
      "epoch: 3726 loss: 0.42689365\n",
      "epoch: 3751 loss: 0.43270463\n",
      "epoch: 3776 loss: 0.42744863\n",
      "epoch: 3801 loss: 0.45015913\n",
      "epoch: 3826 loss: 0.41682050\n",
      "epoch: 3851 loss: 0.43888989\n",
      "epoch: 3876 loss: 0.41596577\n",
      "epoch: 3901 loss: 0.43435279\n",
      "epoch: 3926 loss: 0.41534343\n",
      "epoch: 3951 loss: 0.42132878\n",
      "epoch: 3976 loss: 0.43031105\n",
      "epoch: 4001 loss: 0.41871360\n",
      "epoch: 4026 loss: 0.44112378\n",
      "epoch: 4051 loss: 0.42559665\n",
      "epoch: 4076 loss: 0.42383870\n",
      "epoch: 4101 loss: 0.44024140\n",
      "epoch: 4126 loss: 0.44176081\n",
      "epoch: 4151 loss: 0.40924749\n",
      "epoch: 4176 loss: 0.42430499\n",
      "epoch: 4201 loss: 0.43442407\n",
      "epoch: 4226 loss: 0.42040139\n",
      "epoch: 4251 loss: 0.43188408\n",
      "epoch: 4276 loss: 0.42423862\n",
      "epoch: 4301 loss: 0.42467871\n",
      "epoch: 4326 loss: 0.43271631\n",
      "epoch: 4351 loss: 0.43533614\n",
      "epoch: 4376 loss: 0.41711032\n",
      "epoch: 4401 loss: 0.42657828\n",
      "epoch: 4426 loss: 0.42018107\n",
      "epoch: 4451 loss: 0.41797680\n",
      "epoch: 4476 loss: 0.42752531\n",
      "epoch: 4501 loss: 0.40624666\n",
      "epoch: 4526 loss: 0.42141750\n",
      "epoch: 4551 loss: 0.40939984\n",
      "epoch: 4576 loss: 0.42264080\n",
      "epoch: 4601 loss: 0.43215239\n",
      "epoch: 4626 loss: 0.41606086\n",
      "epoch: 4651 loss: 0.41248357\n",
      "epoch: 4676 loss: 0.43488634\n",
      "epoch: 4701 loss: 0.42167065\n",
      "epoch: 4726 loss: 0.41061643\n",
      "epoch: 4751 loss: 0.42984438\n",
      "epoch: 4776 loss: 0.41731554\n",
      "epoch: 4801 loss: 0.43163186\n",
      "epoch: 4826 loss: 0.41721699\n",
      "epoch: 4851 loss: 0.42493510\n",
      "epoch: 4876 loss: 0.41081402\n",
      "epoch: 4901 loss: 0.42308319\n",
      "epoch: 4926 loss: 0.42855775\n",
      "epoch: 4951 loss: 0.42584246\n",
      "epoch: 4976 loss: 0.40767956\n",
      "epoch: 5001 loss: 0.39167100\n",
      "epoch: 5026 loss: 0.41844335\n",
      "epoch: 5051 loss: 0.42187530\n",
      "epoch: 5076 loss: 0.41789329\n",
      "epoch: 5101 loss: 0.39767298\n",
      "epoch: 5126 loss: 0.42785433\n",
      "epoch: 5151 loss: 0.39970013\n",
      "epoch: 5176 loss: 0.41648969\n",
      "epoch: 5201 loss: 0.40899968\n",
      "epoch: 5226 loss: 0.40640619\n",
      "epoch: 5251 loss: 0.40375757\n",
      "epoch: 5276 loss: 0.41614726\n",
      "epoch: 5301 loss: 0.41468188\n",
      "epoch: 5326 loss: 0.40834460\n",
      "epoch: 5351 loss: 0.40357348\n",
      "epoch: 5376 loss: 0.41502723\n",
      "epoch: 5401 loss: 0.41351807\n",
      "epoch: 5426 loss: 0.42101189\n",
      "epoch: 5451 loss: 0.41350999\n",
      "epoch: 5476 loss: 0.42740092\n",
      "epoch: 5501 loss: 0.40974778\n",
      "epoch: 5526 loss: 0.40697575\n",
      "epoch: 5551 loss: 0.40890735\n",
      "epoch: 5576 loss: 0.38272786\n",
      "epoch: 5601 loss: 0.42080444\n",
      "epoch: 5626 loss: 0.42128050\n",
      "epoch: 5651 loss: 0.40082178\n",
      "epoch: 5676 loss: 0.41275397\n",
      "epoch: 5701 loss: 0.41268262\n",
      "epoch: 5726 loss: 0.41729364\n",
      "epoch: 5751 loss: 0.41152433\n",
      "epoch: 5776 loss: 0.43880633\n",
      "epoch: 5801 loss: 0.42011026\n",
      "epoch: 5826 loss: 0.40289623\n",
      "epoch: 5851 loss: 0.40155667\n",
      "epoch: 5876 loss: 0.39716503\n",
      "epoch: 5901 loss: 0.38741374\n",
      "epoch: 5926 loss: 0.41039053\n",
      "epoch: 5951 loss: 0.42462742\n",
      "epoch: 5976 loss: 0.38659266\n",
      "epoch: 6001 loss: 0.40765265\n",
      "epoch: 6026 loss: 0.41903391\n",
      "epoch: 6051 loss: 0.42029920\n",
      "epoch: 6076 loss: 0.40588835\n",
      "epoch: 6101 loss: 0.39729309\n",
      "epoch: 6126 loss: 0.40794042\n",
      "epoch: 6151 loss: 0.39635646\n",
      "epoch: 6176 loss: 0.41567153\n",
      "epoch: 6201 loss: 0.40144289\n",
      "epoch: 6226 loss: 0.40628627\n",
      "epoch: 6251 loss: 0.40861085\n",
      "epoch: 6276 loss: 0.39864913\n",
      "epoch: 6301 loss: 0.40772936\n",
      "epoch: 6326 loss: 0.40849674\n",
      "epoch: 6351 loss: 0.40993205\n",
      "epoch: 6376 loss: 0.39156282\n",
      "epoch: 6401 loss: 0.38686460\n",
      "epoch: 6426 loss: 0.40056628\n",
      "epoch: 6451 loss: 0.40526065\n",
      "epoch: 6476 loss: 0.37723103\n",
      "epoch: 6501 loss: 0.37876815\n",
      "epoch: 6526 loss: 0.39823350\n",
      "epoch: 6551 loss: 0.41141537\n",
      "epoch: 6576 loss: 0.39901230\n",
      "epoch: 6601 loss: 0.41286916\n",
      "epoch: 6626 loss: 0.41640544\n",
      "epoch: 6651 loss: 0.40714297\n",
      "epoch: 6676 loss: 0.41496372\n",
      "epoch: 6701 loss: 0.38172865\n",
      "epoch: 6726 loss: 0.40238035\n",
      "epoch: 6751 loss: 0.38690242\n",
      "epoch: 6776 loss: 0.40897328\n",
      "epoch: 6801 loss: 0.38811922\n",
      "epoch: 6826 loss: 0.40373024\n",
      "epoch: 6851 loss: 0.41634160\n",
      "epoch: 6876 loss: 0.40046963\n",
      "epoch: 6901 loss: 0.38440737\n",
      "epoch: 6926 loss: 0.38767338\n",
      "epoch: 6951 loss: 0.39221263\n",
      "epoch: 6976 loss: 0.39880216\n",
      "epoch: 7001 loss: 0.38824910\n",
      "epoch: 7026 loss: 0.40316752\n",
      "epoch: 7051 loss: 0.38743070\n",
      "epoch: 7076 loss: 0.39027184\n",
      "epoch: 7101 loss: 0.37676173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7126 loss: 0.41162547\n",
      "epoch: 7151 loss: 0.39522994\n",
      "epoch: 7176 loss: 0.40605253\n",
      "epoch: 7201 loss: 0.39341182\n",
      "epoch: 7226 loss: 0.40299100\n",
      "epoch: 7251 loss: 0.39272180\n",
      "epoch: 7276 loss: 0.38030550\n",
      "epoch: 7301 loss: 0.40726402\n",
      "epoch: 7326 loss: 0.38895378\n",
      "epoch: 7351 loss: 0.42493680\n",
      "epoch: 7376 loss: 0.37836385\n",
      "epoch: 7401 loss: 0.39661828\n",
      "epoch: 7426 loss: 0.39231256\n",
      "epoch: 7451 loss: 0.38331768\n",
      "epoch: 7476 loss: 0.39751291\n",
      "epoch: 7501 loss: 0.38861156\n",
      "epoch: 7526 loss: 0.41570920\n",
      "epoch: 7551 loss: 0.39749393\n",
      "epoch: 7576 loss: 0.39200220\n",
      "epoch: 7601 loss: 0.38624430\n",
      "epoch: 7626 loss: 0.38831657\n",
      "epoch: 7651 loss: 0.36804757\n",
      "epoch: 7676 loss: 0.38639703\n",
      "epoch: 7701 loss: 0.39095077\n",
      "epoch: 7726 loss: 0.38090408\n",
      "epoch: 7751 loss: 0.38171390\n",
      "epoch: 7776 loss: 0.37526843\n",
      "epoch: 7801 loss: 0.38740167\n",
      "epoch: 7826 loss: 0.37251329\n",
      "epoch: 7851 loss: 0.39687827\n",
      "epoch: 7876 loss: 0.37296417\n",
      "epoch: 7901 loss: 0.37441275\n",
      "epoch: 7926 loss: 0.35852718\n",
      "epoch: 7951 loss: 0.38085386\n",
      "epoch: 7976 loss: 0.37009332\n",
      "epoch: 8001 loss: 0.37015188\n",
      "epoch: 8026 loss: 0.39999142\n",
      "epoch: 8051 loss: 0.38624263\n",
      "epoch: 8076 loss: 0.38302708\n",
      "epoch: 8101 loss: 0.37238342\n",
      "epoch: 8126 loss: 0.38464513\n",
      "epoch: 8151 loss: 0.37836426\n",
      "epoch: 8176 loss: 0.38138026\n",
      "epoch: 8201 loss: 0.38576633\n",
      "epoch: 8226 loss: 0.35876262\n",
      "epoch: 8251 loss: 0.39384058\n",
      "epoch: 8276 loss: 0.35244527\n",
      "epoch: 8301 loss: 0.39159107\n",
      "epoch: 8326 loss: 0.36405906\n",
      "epoch: 8351 loss: 0.38770783\n",
      "epoch: 8376 loss: 0.37218654\n",
      "epoch: 8401 loss: 0.39248985\n",
      "epoch: 8426 loss: 0.37878728\n",
      "epoch: 8451 loss: 0.39042515\n",
      "epoch: 8476 loss: 0.37880942\n",
      "epoch: 8501 loss: 0.37597567\n",
      "epoch: 8526 loss: 0.38842976\n",
      "epoch: 8551 loss: 0.37017450\n",
      "epoch: 8576 loss: 0.40384993\n",
      "epoch: 8601 loss: 0.38217860\n",
      "epoch: 8626 loss: 0.37132993\n",
      "epoch: 8651 loss: 0.37348455\n",
      "epoch: 8676 loss: 0.39269567\n",
      "epoch: 8701 loss: 0.36287996\n",
      "epoch: 8726 loss: 0.37892550\n",
      "epoch: 8751 loss: 0.38706723\n",
      "epoch: 8776 loss: 0.38617170\n",
      "epoch: 8801 loss: 0.39083064\n",
      "epoch: 8826 loss: 0.37732890\n",
      "epoch: 8851 loss: 0.37962094\n",
      "epoch: 8876 loss: 0.37397772\n",
      "epoch: 8901 loss: 0.40024015\n",
      "epoch: 8926 loss: 0.39800021\n",
      "epoch: 8951 loss: 0.37036240\n",
      "epoch: 8976 loss: 0.36580005\n",
      "epoch: 9001 loss: 0.41052231\n",
      "epoch: 9026 loss: 0.38867155\n",
      "epoch: 9051 loss: 0.37235630\n",
      "epoch: 9076 loss: 0.34946623\n",
      "epoch: 9101 loss: 0.35422316\n",
      "epoch: 9126 loss: 0.38436162\n",
      "epoch: 9151 loss: 0.35215724\n",
      "epoch: 9176 loss: 0.38914895\n",
      "epoch: 9201 loss: 0.38107273\n",
      "epoch: 9226 loss: 0.36217752\n",
      "epoch: 9251 loss: 0.37745073\n",
      "epoch: 9276 loss: 0.38415948\n",
      "epoch: 9301 loss: 0.40299416\n",
      "epoch: 9326 loss: 0.38753089\n",
      "epoch: 9351 loss: 0.38424462\n",
      "epoch: 9376 loss: 0.37342706\n",
      "epoch: 9401 loss: 0.34580693\n",
      "epoch: 9426 loss: 0.36215678\n",
      "epoch: 9451 loss: 0.36887679\n",
      "epoch: 9476 loss: 0.38239416\n",
      "epoch: 9501 loss: 0.38911867\n",
      "epoch: 9526 loss: 0.36522025\n",
      "epoch: 9551 loss: 0.39469334\n",
      "epoch: 9576 loss: 0.35303617\n",
      "epoch: 9601 loss: 0.36188030\n",
      "epoch: 9626 loss: 0.38629875\n",
      "epoch: 9651 loss: 0.36480412\n",
      "epoch: 9676 loss: 0.38145339\n",
      "epoch: 9701 loss: 0.33900145\n",
      "epoch: 9726 loss: 0.37108332\n",
      "epoch: 9751 loss: 0.34366792\n",
      "epoch: 9776 loss: 0.36662549\n",
      "epoch: 9801 loss: 0.36468667\n",
      "epoch: 9826 loss: 0.37000936\n",
      "epoch: 9851 loss: 0.38319132\n",
      "epoch: 9876 loss: 0.38175660\n",
      "epoch: 9901 loss: 0.38152894\n",
      "epoch: 9926 loss: 0.36498812\n",
      "epoch: 9951 loss: 0.37034243\n",
      "epoch: 9976 loss: 0.34067023\n",
      "epoch: 10001 loss: 0.35395232\n",
      "epoch: 10026 loss: 0.39026678\n",
      "epoch: 10051 loss: 0.36645749\n",
      "epoch: 10076 loss: 0.34712741\n",
      "epoch: 10101 loss: 0.36099771\n",
      "epoch: 10126 loss: 0.38668349\n",
      "epoch: 10151 loss: 0.39270443\n",
      "epoch: 10176 loss: 0.37091911\n",
      "epoch: 10201 loss: 0.36315992\n",
      "epoch: 10226 loss: 0.35352182\n",
      "epoch: 10251 loss: 0.38447315\n",
      "epoch: 10276 loss: 0.35473400\n",
      "epoch: 10301 loss: 0.36458969\n",
      "epoch: 10326 loss: 0.34085479\n",
      "epoch: 10351 loss: 0.36396250\n",
      "epoch: 10376 loss: 0.38338625\n",
      "epoch: 10401 loss: 0.34409210\n",
      "epoch: 10426 loss: 0.36070809\n",
      "epoch: 10451 loss: 0.34271213\n",
      "epoch: 10476 loss: 0.35185766\n",
      "epoch: 10501 loss: 0.33641708\n",
      "epoch: 10526 loss: 0.37789360\n",
      "epoch: 10551 loss: 0.37421900\n",
      "epoch: 10576 loss: 0.36245722\n",
      "epoch: 10601 loss: 0.37138295\n",
      "epoch: 10626 loss: 0.37214044\n",
      "epoch: 10651 loss: 0.37205070\n",
      "epoch: 10676 loss: 0.33475405\n",
      "epoch: 10701 loss: 0.36998373\n",
      "epoch: 10726 loss: 0.36467907\n",
      "epoch: 10751 loss: 0.34819531\n",
      "epoch: 10776 loss: 0.36803746\n",
      "epoch: 10801 loss: 0.37543270\n",
      "epoch: 10826 loss: 0.35816312\n",
      "epoch: 10851 loss: 0.38264588\n",
      "epoch: 10876 loss: 0.36780477\n",
      "epoch: 10901 loss: 0.34785238\n",
      "epoch: 10926 loss: 0.36425653\n",
      "epoch: 10951 loss: 0.34987861\n",
      "epoch: 10976 loss: 0.34194657\n",
      "epoch: 11001 loss: 0.37191188\n",
      "epoch: 11026 loss: 0.36005828\n",
      "epoch: 11051 loss: 0.34207579\n",
      "epoch: 11076 loss: 0.32869717\n",
      "epoch: 11101 loss: 0.38809487\n",
      "epoch: 11126 loss: 0.34964740\n",
      "epoch: 11151 loss: 0.34825942\n",
      "epoch: 11176 loss: 0.37150955\n",
      "epoch: 11201 loss: 0.37710503\n",
      "epoch: 11226 loss: 0.34135148\n",
      "epoch: 11251 loss: 0.35809213\n",
      "epoch: 11276 loss: 0.34466669\n",
      "epoch: 11301 loss: 0.36635667\n",
      "epoch: 11326 loss: 0.36243346\n",
      "epoch: 11351 loss: 0.36121172\n",
      "epoch: 11376 loss: 0.35626933\n",
      "epoch: 11401 loss: 0.35460085\n",
      "epoch: 11426 loss: 0.34266990\n",
      "epoch: 11451 loss: 0.35976127\n",
      "epoch: 11476 loss: 0.36261994\n",
      "epoch: 11501 loss: 0.35630074\n",
      "epoch: 11526 loss: 0.35039100\n",
      "epoch: 11551 loss: 0.34142610\n",
      "epoch: 11576 loss: 0.35596967\n",
      "epoch: 11601 loss: 0.36427096\n",
      "epoch: 11626 loss: 0.35225290\n",
      "epoch: 11651 loss: 0.32894570\n",
      "epoch: 11676 loss: 0.32968074\n",
      "epoch: 11701 loss: 0.34398988\n",
      "epoch: 11726 loss: 0.37415323\n",
      "epoch: 11751 loss: 0.33554617\n",
      "epoch: 11776 loss: 0.36511913\n",
      "epoch: 11801 loss: 0.34262520\n",
      "epoch: 11826 loss: 0.34970757\n",
      "epoch: 11851 loss: 0.32326600\n",
      "epoch: 11876 loss: 0.36543241\n",
      "epoch: 11901 loss: 0.32457682\n",
      "epoch: 11926 loss: 0.37042540\n",
      "epoch: 11951 loss: 0.38538000\n",
      "epoch: 11976 loss: 0.36860007\n",
      "epoch: 12001 loss: 0.34367898\n",
      "epoch: 12026 loss: 0.36579537\n",
      "epoch: 12051 loss: 0.34961298\n",
      "epoch: 12076 loss: 0.36256701\n",
      "epoch: 12101 loss: 0.35908338\n",
      "epoch: 12126 loss: 0.34439069\n",
      "epoch: 12151 loss: 0.35038975\n",
      "epoch: 12176 loss: 0.33857894\n",
      "epoch: 12201 loss: 0.36657003\n",
      "epoch: 12226 loss: 0.34035042\n",
      "epoch: 12251 loss: 0.34621286\n",
      "epoch: 12276 loss: 0.34912038\n",
      "epoch: 12301 loss: 0.31594267\n",
      "epoch: 12326 loss: 0.36720645\n",
      "epoch: 12351 loss: 0.33173838\n",
      "epoch: 12376 loss: 0.34972411\n",
      "epoch: 12401 loss: 0.32996109\n",
      "epoch: 12426 loss: 0.33725619\n",
      "epoch: 12451 loss: 0.34550118\n",
      "epoch: 12476 loss: 0.32238916\n",
      "epoch: 12501 loss: 0.34510133\n",
      "epoch: 12526 loss: 0.35138306\n",
      "epoch: 12551 loss: 0.35593671\n",
      "epoch: 12576 loss: 0.34683043\n",
      "epoch: 12601 loss: 0.33675411\n",
      "epoch: 12626 loss: 0.34595549\n",
      "epoch: 12651 loss: 0.33295926\n",
      "epoch: 12676 loss: 0.32086390\n",
      "epoch: 12701 loss: 0.34425920\n",
      "epoch: 12726 loss: 0.32912910\n",
      "epoch: 12751 loss: 0.32162353\n",
      "epoch: 12776 loss: 0.33646724\n",
      "epoch: 12801 loss: 0.34611136\n",
      "epoch: 12826 loss: 0.34858626\n",
      "epoch: 12851 loss: 0.33744967\n",
      "epoch: 12876 loss: 0.33690166\n",
      "epoch: 12901 loss: 0.34413710\n",
      "epoch: 12926 loss: 0.33025873\n",
      "epoch: 12951 loss: 0.32088998\n",
      "epoch: 12976 loss: 0.34200752\n",
      "epoch: 13001 loss: 0.34698492\n",
      "epoch: 13026 loss: 0.31900501\n",
      "epoch: 13051 loss: 0.33215761\n",
      "epoch: 13076 loss: 0.35113752\n",
      "epoch: 13101 loss: 0.33285061\n",
      "epoch: 13126 loss: 0.34457061\n",
      "epoch: 13151 loss: 0.32659623\n",
      "epoch: 13176 loss: 0.31151518\n",
      "epoch: 13201 loss: 0.32506621\n",
      "epoch: 13226 loss: 0.31330979\n",
      "epoch: 13251 loss: 0.33516675\n",
      "epoch: 13276 loss: 0.35603014\n",
      "epoch: 13301 loss: 0.35169676\n",
      "epoch: 13326 loss: 0.32609007\n",
      "epoch: 13351 loss: 0.34182575\n",
      "epoch: 13376 loss: 0.35592821\n",
      "epoch: 13401 loss: 0.33516875\n",
      "epoch: 13426 loss: 0.34899500\n",
      "epoch: 13451 loss: 0.31829250\n",
      "epoch: 13476 loss: 0.33663934\n",
      "epoch: 13501 loss: 0.33985928\n",
      "epoch: 13526 loss: 0.32457563\n",
      "epoch: 13551 loss: 0.30105904\n",
      "epoch: 13576 loss: 0.33335865\n",
      "epoch: 13601 loss: 0.32068363\n",
      "epoch: 13626 loss: 0.34502995\n",
      "epoch: 13651 loss: 0.35092258\n",
      "epoch: 13676 loss: 0.32627264\n",
      "epoch: 13701 loss: 0.35618111\n",
      "epoch: 13726 loss: 0.35634637\n",
      "epoch: 13751 loss: 0.32957128\n",
      "epoch: 13776 loss: 0.34144780\n",
      "epoch: 13801 loss: 0.33724469\n",
      "epoch: 13826 loss: 0.33532625\n",
      "epoch: 13851 loss: 0.34346598\n",
      "epoch: 13876 loss: 0.32432956\n",
      "epoch: 13901 loss: 0.33674565\n",
      "epoch: 13926 loss: 0.35398662\n",
      "epoch: 13951 loss: 0.31672302\n",
      "epoch: 13976 loss: 0.35306442\n",
      "epoch: 14001 loss: 0.31150264\n",
      "epoch: 14026 loss: 0.32981867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14051 loss: 0.30238032\n",
      "epoch: 14076 loss: 0.30881089\n",
      "epoch: 14101 loss: 0.30277127\n",
      "epoch: 14126 loss: 0.29883534\n",
      "epoch: 14151 loss: 0.31569558\n",
      "epoch: 14176 loss: 0.31851259\n",
      "epoch: 14201 loss: 0.32955855\n",
      "epoch: 14226 loss: 0.32294106\n",
      "epoch: 14251 loss: 0.31553134\n",
      "epoch: 14276 loss: 0.29600471\n",
      "epoch: 14301 loss: 0.34810576\n",
      "epoch: 14326 loss: 0.34474409\n",
      "epoch: 14351 loss: 0.29201445\n",
      "epoch: 14376 loss: 0.34080014\n",
      "epoch: 14401 loss: 0.32965249\n",
      "epoch: 14426 loss: 0.31219721\n",
      "epoch: 14451 loss: 0.32600152\n",
      "epoch: 14476 loss: 0.30810276\n",
      "epoch: 14501 loss: 0.30783957\n",
      "epoch: 14526 loss: 0.32098442\n",
      "epoch: 14551 loss: 0.32346919\n",
      "epoch: 14576 loss: 0.34977940\n",
      "epoch: 14601 loss: 0.38288289\n",
      "epoch: 14626 loss: 0.30446425\n",
      "epoch: 14651 loss: 0.32216936\n",
      "epoch: 14676 loss: 0.32334775\n",
      "epoch: 14701 loss: 0.30047119\n",
      "epoch: 14726 loss: 0.34497711\n",
      "epoch: 14751 loss: 0.34260792\n",
      "epoch: 14776 loss: 0.32682589\n",
      "epoch: 14801 loss: 0.31360558\n",
      "epoch: 14826 loss: 0.34289911\n",
      "epoch: 14851 loss: 0.32878727\n",
      "epoch: 14876 loss: 0.33878848\n",
      "epoch: 14901 loss: 0.32363179\n",
      "epoch: 14926 loss: 0.37456316\n",
      "epoch: 14951 loss: 0.31037468\n",
      "epoch: 14976 loss: 0.32433397\n",
      "epoch: 15001 loss: 0.29925457\n",
      "epoch: 15026 loss: 0.29145306\n",
      "epoch: 15051 loss: 0.33594024\n",
      "epoch: 15076 loss: 0.31250688\n",
      "epoch: 15101 loss: 0.31106678\n",
      "epoch: 15126 loss: 0.33494726\n",
      "epoch: 15151 loss: 0.32097319\n",
      "epoch: 15176 loss: 0.28931746\n",
      "epoch: 15201 loss: 0.30043328\n",
      "epoch: 15226 loss: 0.32654825\n",
      "epoch: 15251 loss: 0.33044171\n",
      "epoch: 15276 loss: 0.30726585\n",
      "epoch: 15301 loss: 0.34827659\n",
      "epoch: 15326 loss: 0.30568010\n",
      "epoch: 15351 loss: 0.33183685\n",
      "epoch: 15376 loss: 0.28292537\n",
      "epoch: 15401 loss: 0.28945616\n",
      "epoch: 15426 loss: 0.32331821\n",
      "epoch: 15451 loss: 0.32367718\n",
      "epoch: 15476 loss: 0.35799256\n",
      "epoch: 15501 loss: 0.33252668\n",
      "epoch: 15526 loss: 0.28791285\n",
      "epoch: 15551 loss: 0.32119575\n",
      "epoch: 15576 loss: 0.30868861\n",
      "epoch: 15601 loss: 0.32585582\n",
      "epoch: 15626 loss: 0.30053332\n",
      "epoch: 15651 loss: 0.28288245\n",
      "epoch: 15676 loss: 0.30949831\n",
      "epoch: 15701 loss: 0.27784485\n",
      "epoch: 15726 loss: 0.30819914\n",
      "epoch: 15751 loss: 0.28486931\n",
      "epoch: 15776 loss: 0.32416460\n",
      "epoch: 15801 loss: 0.29625586\n",
      "epoch: 15826 loss: 0.34491315\n",
      "epoch: 15851 loss: 0.33691266\n",
      "epoch: 15876 loss: 0.32106656\n",
      "epoch: 15901 loss: 0.35449320\n",
      "epoch: 15926 loss: 0.32737848\n",
      "epoch: 15951 loss: 0.32571918\n",
      "epoch: 15976 loss: 0.31105983\n",
      "epoch: 16001 loss: 0.33011296\n",
      "epoch: 16026 loss: 0.31254587\n",
      "epoch: 16051 loss: 0.30175859\n",
      "epoch: 16076 loss: 0.31655490\n",
      "epoch: 16101 loss: 0.30441350\n",
      "epoch: 16126 loss: 0.31253988\n",
      "epoch: 16151 loss: 0.29415998\n",
      "epoch: 16176 loss: 0.33594260\n",
      "epoch: 16201 loss: 0.32490611\n",
      "epoch: 16226 loss: 0.30228943\n",
      "epoch: 16251 loss: 0.31485665\n",
      "epoch: 16276 loss: 0.33203292\n",
      "epoch: 16301 loss: 0.31314912\n",
      "epoch: 16326 loss: 0.30859345\n",
      "epoch: 16351 loss: 0.31185162\n",
      "epoch: 16376 loss: 0.32510352\n",
      "epoch: 16401 loss: 0.30046934\n",
      "epoch: 16426 loss: 0.31755543\n",
      "epoch: 16451 loss: 0.26637593\n",
      "epoch: 16476 loss: 0.31676093\n",
      "epoch: 16501 loss: 0.29514274\n",
      "epoch: 16526 loss: 0.28977358\n",
      "epoch: 16551 loss: 0.32091570\n",
      "epoch: 16576 loss: 0.33134392\n",
      "epoch: 16601 loss: 0.34720993\n",
      "epoch: 16626 loss: 0.30018172\n",
      "epoch: 16651 loss: 0.29450321\n",
      "epoch: 16676 loss: 0.36638951\n",
      "epoch: 16701 loss: 0.29715103\n",
      "epoch: 16726 loss: 0.29527009\n",
      "epoch: 16751 loss: 0.34334770\n",
      "epoch: 16776 loss: 0.33977365\n",
      "epoch: 16801 loss: 0.33739156\n",
      "epoch: 16826 loss: 0.30430317\n",
      "epoch: 16851 loss: 0.30106539\n",
      "epoch: 16876 loss: 0.31146431\n",
      "epoch: 16901 loss: 0.30641407\n",
      "epoch: 16926 loss: 0.29943866\n",
      "epoch: 16951 loss: 0.31443861\n",
      "epoch: 16976 loss: 0.32204011\n",
      "epoch: 17001 loss: 0.30178538\n",
      "epoch: 17026 loss: 0.30309224\n",
      "epoch: 17051 loss: 0.30580056\n",
      "epoch: 17076 loss: 0.28967577\n",
      "epoch: 17101 loss: 0.32309264\n",
      "epoch: 17126 loss: 0.29212287\n",
      "epoch: 17151 loss: 0.31011939\n",
      "epoch: 17176 loss: 0.27543426\n",
      "epoch: 17201 loss: 0.31569049\n",
      "epoch: 17226 loss: 0.33058321\n",
      "epoch: 17251 loss: 0.26822934\n",
      "epoch: 17276 loss: 0.33105090\n",
      "epoch: 17301 loss: 0.30075797\n",
      "epoch: 17326 loss: 0.31177816\n",
      "epoch: 17351 loss: 0.34208655\n",
      "epoch: 17376 loss: 0.30470687\n",
      "epoch: 17401 loss: 0.30800083\n",
      "epoch: 17426 loss: 0.29775271\n",
      "epoch: 17451 loss: 0.26278546\n",
      "epoch: 17476 loss: 0.32289565\n",
      "epoch: 17501 loss: 0.31822583\n",
      "epoch: 17526 loss: 0.26044548\n",
      "epoch: 17551 loss: 0.28927785\n",
      "epoch: 17576 loss: 0.31020984\n",
      "epoch: 17601 loss: 0.29610023\n",
      "epoch: 17626 loss: 0.31561559\n",
      "epoch: 17651 loss: 0.30132827\n",
      "epoch: 17676 loss: 0.32266423\n",
      "epoch: 17701 loss: 0.28792301\n",
      "epoch: 17726 loss: 0.29844400\n",
      "epoch: 17751 loss: 0.30266327\n",
      "epoch: 17776 loss: 0.25774956\n",
      "epoch: 17801 loss: 0.30934125\n",
      "epoch: 17826 loss: 0.29806709\n",
      "epoch: 17851 loss: 0.29657739\n",
      "epoch: 17876 loss: 0.31361493\n",
      "epoch: 17901 loss: 0.27887228\n",
      "epoch: 17926 loss: 0.31805533\n",
      "epoch: 17951 loss: 0.33195820\n",
      "epoch: 17976 loss: 0.29729280\n",
      "epoch: 18001 loss: 0.30415002\n",
      "epoch: 18026 loss: 0.31214675\n",
      "epoch: 18051 loss: 0.32189515\n",
      "epoch: 18076 loss: 0.28005907\n",
      "epoch: 18101 loss: 0.27751037\n",
      "epoch: 18126 loss: 0.31469476\n",
      "epoch: 18151 loss: 0.32182735\n",
      "epoch: 18176 loss: 0.29778427\n",
      "epoch: 18201 loss: 0.29570612\n",
      "epoch: 18226 loss: 0.28724480\n",
      "epoch: 18251 loss: 0.28415939\n",
      "epoch: 18276 loss: 0.30461574\n",
      "epoch: 18301 loss: 0.30356652\n",
      "epoch: 18326 loss: 0.31576145\n",
      "epoch: 18351 loss: 0.34457177\n",
      "epoch: 18376 loss: 0.33237290\n",
      "epoch: 18401 loss: 0.28981587\n",
      "epoch: 18426 loss: 0.31443679\n",
      "epoch: 18451 loss: 0.29088703\n",
      "epoch: 18476 loss: 0.29765698\n",
      "epoch: 18501 loss: 0.26712513\n",
      "epoch: 18526 loss: 0.30401576\n",
      "epoch: 18551 loss: 0.28517070\n",
      "epoch: 18576 loss: 0.27502504\n",
      "epoch: 18601 loss: 0.28791380\n",
      "epoch: 18626 loss: 0.29721004\n",
      "epoch: 18651 loss: 0.28807643\n",
      "epoch: 18676 loss: 0.28767917\n",
      "epoch: 18701 loss: 0.27000421\n",
      "epoch: 18726 loss: 0.27424923\n",
      "epoch: 18751 loss: 0.31795725\n",
      "epoch: 18776 loss: 0.29372001\n",
      "epoch: 18801 loss: 0.28345102\n",
      "epoch: 18826 loss: 0.29343641\n",
      "epoch: 18851 loss: 0.27158043\n",
      "epoch: 18876 loss: 0.31893727\n",
      "epoch: 18901 loss: 0.30048838\n",
      "epoch: 18926 loss: 0.29177144\n",
      "epoch: 18951 loss: 0.29220948\n",
      "epoch: 18976 loss: 0.28812331\n",
      "epoch: 19001 loss: 0.27216479\n",
      "epoch: 19026 loss: 0.28146574\n",
      "epoch: 19051 loss: 0.29010910\n",
      "epoch: 19076 loss: 0.31099302\n",
      "epoch: 19101 loss: 0.28417158\n",
      "epoch: 19126 loss: 0.33689663\n",
      "epoch: 19151 loss: 0.29453105\n",
      "epoch: 19176 loss: 0.33425438\n",
      "epoch: 19201 loss: 0.26751444\n",
      "epoch: 19226 loss: 0.28655940\n",
      "epoch: 19251 loss: 0.30208704\n",
      "epoch: 19276 loss: 0.30658260\n",
      "epoch: 19301 loss: 0.29434672\n",
      "epoch: 19326 loss: 0.30529797\n",
      "epoch: 19351 loss: 0.28410548\n",
      "epoch: 19376 loss: 0.27408826\n",
      "epoch: 19401 loss: 0.29885206\n",
      "epoch: 19426 loss: 0.29915577\n",
      "epoch: 19451 loss: 0.30572942\n",
      "epoch: 19476 loss: 0.27376929\n",
      "epoch: 19501 loss: 0.31994605\n",
      "epoch: 19526 loss: 0.27656442\n",
      "epoch: 19551 loss: 0.28660476\n",
      "epoch: 19576 loss: 0.25122371\n",
      "epoch: 19601 loss: 0.31451914\n",
      "epoch: 19626 loss: 0.29744864\n",
      "epoch: 19651 loss: 0.29104790\n",
      "epoch: 19676 loss: 0.26391396\n",
      "epoch: 19701 loss: 0.29964539\n",
      "epoch: 19726 loss: 0.25191885\n",
      "epoch: 19751 loss: 0.33319411\n",
      "epoch: 19776 loss: 0.30222225\n",
      "epoch: 19801 loss: 0.32932895\n",
      "epoch: 19826 loss: 0.25983396\n",
      "epoch: 19851 loss: 0.29647702\n",
      "epoch: 19876 loss: 0.29104558\n",
      "epoch: 19901 loss: 0.29526672\n",
      "epoch: 19926 loss: 0.30367434\n",
      "epoch: 19951 loss: 0.31416792\n",
      "epoch: 19976 loss: 0.29141814\n",
      "epoch: 20000 loss: 0.2834869921\n"
     ]
    }
   ],
   "source": [
    "model = Model(numerical_data.shape[1], 2, [100,50,25], p=0.4)\n",
    "\n",
    "#loss_function = nn.CrossEntropyLoss(weight = torch.Tensor([1.0, 1.1]))\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#loss_function = nn.NLLLoss()\n",
    "#loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=5e-4, momentum=.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "epochs = 20000\n",
    "aggregated_losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(numerical_train_data)\n",
    "    single_loss = loss_function(y_pred, train_outputs)\n",
    "    aggregated_losses.append(single_loss)\n",
    "\n",
    "    if i%25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.38791418\n"
     ]
    }
   ],
   "source": [
    "#prints out validation loss\n",
    "with torch.no_grad():\n",
    "    validation_y_val = model(numerical_test_data)\n",
    "    loss = loss_function(validation_y_val, test_outputs)\n",
    "print(f'Loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.29698852\n"
     ]
    }
   ],
   "source": [
    "#prints out training loss\n",
    "with torch.no_grad():\n",
    "    train_y_val = model(numerical_train_data)\n",
    "    loss = loss_function(train_y_val, train_outputs)\n",
    "print(f'Loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.7126436781609196 - F1 Score =  0.7121118885368947\n",
      "Train Accuracy:  0.8631790744466801 - F1 Score =  0.8632361746005475\n"
     ]
    }
   ],
   "source": [
    "validation_y_output = np.argmax(validation_y_val, axis=1)\n",
    "train_y_output = np.argmax(train_y_val, axis=1) \n",
    "\n",
    "validation_accuracy = np.mean(validation_y_output.numpy() == test_outputs.numpy())\n",
    "validation_f1 = f1_score(validation_y_output.numpy(), test_outputs.numpy(), average='weighted')\n",
    "\n",
    "train_accuracy = np.mean(train_y_output.numpy() == train_outputs.numpy())\n",
    "train_f1 = f1_score(train_y_output.numpy(), train_outputs.numpy(), average='weighted')\n",
    "\n",
    "print(\"Validation Accuracy: \", validation_accuracy, \"- F1 Score = \", validation_f1)\n",
    "print(\"Train Accuracy: \", train_accuracy, \"- F1 Score = \", train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set (F1 Score Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = read_data(\"/Users/anthonycuturrufo/Documents/School/Artzi_Eval/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = add_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_numerical_data = np.stack([test_dataset[col].values for col in numerical_columns], 1)\n",
    "testset_numerical_data = torch.tensor(testset_numerical_data, dtype=torch.float)\n",
    "\n",
    "testset_outputs = torch.tensor(test_dataset[output_label].values).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.61506683\n"
     ]
    }
   ],
   "source": [
    "#prints out testing loss\n",
    "with torch.no_grad():\n",
    "    y_val = model(testset_numerical_data)\n",
    "    loss = loss_function(y_val, testset_outputs)\n",
    "print(f'Loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.7701149425287356 - F1 Score =  0.7712201591511936\n"
     ]
    }
   ],
   "source": [
    "y_output = np.argmax(y_val, axis=1)\n",
    "accuracy = np.mean(y_output.numpy() == testset_outputs.numpy())\n",
    "f1 = f1_score(y_output.numpy(), testset_outputs.numpy(), average='weighted')\n",
    "print(\"Testing Accuracy: \",accuracy, \"- F1 Score = \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[58 24]\n",
      " [16 76]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.74        82\n",
      "           1       0.76      0.83      0.79        92\n",
      "\n",
      "    accuracy                           0.77       174\n",
      "   macro avg       0.77      0.77      0.77       174\n",
      "weighted avg       0.77      0.77      0.77       174\n",
      "\n",
      "0.7701149425287356\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(testset_outputs,y_output))\n",
    "print(classification_report(testset_outputs,y_output))\n",
    "print(accuracy_score(testset_outputs, y_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS 4780 Final Project Student Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
