{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mQDt73qwWNp1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import string\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from nltk.translate import nist_score\n",
    "from nltk.translate import gleu_score\n",
    "from nltk.translate import ribes_score\n",
    "from nltk.translate import chrf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(current_file: str):\n",
    "    text_data = list()\n",
    "    if os.path.exists(current_file):\n",
    "        open_file = open(current_file, 'r', encoding=\"utf-8\")\n",
    "        f = open_file.read().split('\\n')  \n",
    "        list_data = [f[x:x+5] for x in range(0, len(f), 6)]\n",
    "        df = pd.DataFrame(list_data, columns =['Source', 'Reference', \"Candidate\", \"Bleu_Score\", \"Label\" ]) \n",
    "        df[\"Bleu_Score\"] = df[\"Bleu_Score\"].apply(pd.to_numeric)\n",
    "\n",
    "    else: \"Error: file does not exist\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_nist(row):\n",
    "    return nist_score.corpus_nist([[row[\"Reference\"].split(\" \")]], [row[\"Candidate\"].split(\" \")])\n",
    "\n",
    "def calc_gleu(row):\n",
    "    return gleu_score.corpus_gleu([[row[\"Reference\"].split(\" \")]], [row[\"Candidate\"].split(\" \")])\n",
    "\n",
    "def calc_ribes(row):\n",
    "    return ribes_score.corpus_ribes([[row[\"Reference\"].split(\" \")]], [row[\"Candidate\"].split(\" \")])\n",
    "\n",
    "def calc_chrf(row):\n",
    "    return chrf_score.corpus_chrf([row[\"Reference\"].split(\" \")], [row[\"Candidate\"].split(\" \")])\n",
    "\n",
    "def calc_avmax(row):\n",
    "    model = similarity_score.load_model()\n",
    "    score = similarity_score.score(row[\"Reference\"],row[\"Candidate\"],model)\n",
    "    \n",
    "# def calc_wer(row, print_matrix=False):\n",
    "#     hyp = row[\"Candidate\"]\n",
    "#     ref = row[\"Reference\"]\n",
    "#     N = len(hyp)\n",
    "#     M = len(ref)\n",
    "#     L = np.zeros((N,M))\n",
    "#     for i in range(0, N):\n",
    "#         for j in range(0, M):\n",
    "#             if min(i,j) == 0:\n",
    "#                 L[i,j] = max(i,j)\n",
    "#             else:\n",
    "#                 deletion = L[i-1,j] + 1\n",
    "#                 insertion = L[i,j-1] + 1\n",
    "#                 sub = 1 if hyp[i] != ref[j] else 0\n",
    "#                 substitution = L[i-1,j-1] + sub\n",
    "#                 L[i,j] = min(deletion, min(insertion, substitution))\n",
    "#                 # print(\"{} - {}: del {} ins {} sub {} s {}\".format(hyp[i], ref[j], deletion, insertion, substitution, sub))\n",
    "#     if print_matrix:\n",
    "#         print(\"WER matrix ({}x{}): \".format(N, M))\n",
    "#         print(L)\n",
    "#     return int(L[N-1, M-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df[\"Ref_Length\"] = df[\"Reference\"].apply(lambda x: len(x))\n",
    "    df[\"Can_Length\"] = df[\"Candidate\"].apply(lambda x: len(x))\n",
    "    df[\"Src_Length\"] = df[\"Source\"].apply(lambda x: len(x))\n",
    "    df[\"target\"] = df[\"Label\"].apply(lambda x: 1 if x == \"H\" else 0)\n",
    "    \n",
    "    df[\"NIST_Score\"] = df.apply(lambda row: calc_nist(row), axis=1)\n",
    "    df[\"Gleu_Score\"] = df.apply(lambda row: calc_gleu(row), axis=1)\n",
    "#   df[\"Ribes_Score\"] = df.apply(lambda row: calc_ribes(row), axis=1)\n",
    "    df[\"Chrf_Score\"] = df.apply(lambda row: calc_chrf(row), axis=1)\n",
    "#   df[\"AvMax_Score\"] = df.apply(lambda row: calc_avmax(row), axis=1)\n",
    "#   df[\"Wer_Score\"] = df.apply(lambda row: calc_wer(row), axis=1)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data(\"../data/train.txt\")\n",
    "dataset = add_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Candidate</th>\n",
       "      <th>Bleu_Score</th>\n",
       "      <th>Label</th>\n",
       "      <th>Ref_Length</th>\n",
       "      <th>Can_Length</th>\n",
       "      <th>Src_Length</th>\n",
       "      <th>target</th>\n",
       "      <th>NIST_Score</th>\n",
       "      <th>Gleu_Score</th>\n",
       "      <th>Chrf_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>巴林 公主 下 嫁 美 大兵 惊 世 婚姻 五 年 宣告 破裂</td>\n",
       "      <td>bahraini princess marries us soldier , astonis...</td>\n",
       "      <td>bahraini princess marries a u.s. soldier ; ast...</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>H</td>\n",
       "      <td>75</td>\n",
       "      <td>83</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1.423246</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.539032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>巴林 公主 梅 丽 安 ・ 哈 里 发 下 嫁 美国 陆 战 队 大兵 强 生 , 曾 获 ...</td>\n",
       "      <td>the star-crossed marriage between bahraini pri...</td>\n",
       "      <td>u.s. television stations had once feted the ma...</td>\n",
       "      <td>0.6531</td>\n",
       "      <td>H</td>\n",
       "      <td>234</td>\n",
       "      <td>251</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>3.941611</td>\n",
       "      <td>0.427711</td>\n",
       "      <td>0.700914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>梅 丽 安 是 海湾 国家 巴林 王室 的 成员 , 强 生 冒 着 赔 上 军旅 生涯 的...</td>\n",
       "      <td>meriam is a member of the gulf country bahrain...</td>\n",
       "      <td>meri gulf state of bahrain , the royal family ...</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>M</td>\n",
       "      <td>198</td>\n",
       "      <td>183</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>1.663802</td>\n",
       "      <td>0.102740</td>\n",
       "      <td>0.350299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>不过 , 据 拉 斯 维 加 斯 评论 报 报导 , 才 过 了 五 年 , 两 人 就 对...</td>\n",
       "      <td>but the las vegas review-journal reported that...</td>\n",
       "      <td>however , according to the las vegas , comment...</td>\n",
       "      <td>0.3646</td>\n",
       "      <td>M</td>\n",
       "      <td>220</td>\n",
       "      <td>174</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>1.998639</td>\n",
       "      <td>0.171233</td>\n",
       "      <td>0.361169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>他们 两 人 在 一九九九年 相遇 , 当时 强 生 还是 职业 军人 , 派 驻 在 巴林 .</td>\n",
       "      <td>the pair met in 1999 when career military man ...</td>\n",
       "      <td>the two met in 1999 , when johnson was still a...</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>H</td>\n",
       "      <td>80</td>\n",
       "      <td>89</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>3.045229</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.782057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Source  \\\n",
       "0                    巴林 公主 下 嫁 美 大兵 惊 世 婚姻 五 年 宣告 破裂   \n",
       "1  巴林 公主 梅 丽 安 ・ 哈 里 发 下 嫁 美国 陆 战 队 大兵 强 生 , 曾 获 ...   \n",
       "2  梅 丽 安 是 海湾 国家 巴林 王室 的 成员 , 强 生 冒 着 赔 上 军旅 生涯 的...   \n",
       "3  不过 , 据 拉 斯 维 加 斯 评论 报 报导 , 才 过 了 五 年 , 两 人 就 对...   \n",
       "4   他们 两 人 在 一九九九年 相遇 , 当时 强 生 还是 职业 军人 , 派 驻 在 巴林 .   \n",
       "\n",
       "                                           Reference  \\\n",
       "0  bahraini princess marries us soldier , astonis...   \n",
       "1  the star-crossed marriage between bahraini pri...   \n",
       "2  meriam is a member of the gulf country bahrain...   \n",
       "3  but the las vegas review-journal reported that...   \n",
       "4  the pair met in 1999 when career military man ...   \n",
       "\n",
       "                                           Candidate  Bleu_Score Label  \\\n",
       "0  bahraini princess marries a u.s. soldier ; ast...      0.3125     H   \n",
       "1  u.s. television stations had once feted the ma...      0.6531     H   \n",
       "2  meri gulf state of bahrain , the royal family ...      0.3784     M   \n",
       "3  however , according to the las vegas , comment...      0.3646     M   \n",
       "4  the two met in 1999 , when johnson was still a...      0.7778     H   \n",
       "\n",
       "   Ref_Length  Can_Length  Src_Length  target  NIST_Score  Gleu_Score  \\\n",
       "0          75          83          31       1    1.423246    0.173913   \n",
       "1         234         251         129       1    3.941611    0.427711   \n",
       "2         198         183         111       0    1.663802    0.102740   \n",
       "3         220         174         106       0    1.998639    0.171233   \n",
       "4          80          89          48       1    3.045229    0.409091   \n",
       "\n",
       "   Chrf_Score  \n",
       "0    0.539032  \n",
       "1    0.700914  \n",
       "2    0.350299  \n",
       "3    0.361169  \n",
       "4    0.782057  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['Chrf_Score','Gleu_Score','Bleu_Score','Ref_Length','Can_Length','Src_Length']\n",
    "output_label = ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = np.stack([dataset[col].values for col in numerical_columns], 1)\n",
    "numerical_data = torch.tensor(numerical_data, dtype=torch.float)\n",
    "outputs = torch.tensor(dataset[output_label].values).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records = 584\n",
    "test_records = int(total_records * .15)\n",
    "\n",
    "numerical_train_data = numerical_data[:total_records-test_records]\n",
    "numerical_test_data = numerical_data[total_records-test_records:total_records]\n",
    "train_outputs = outputs[:total_records-test_records]\n",
    "test_outputs = outputs[total_records-test_records:total_records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "        input_size = num_numerical_cols\n",
    "        all_layers = []\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "        \n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "        \n",
    "    def forward(self, x_numerical):     \n",
    "        x_numerical = self.batch_norm_num(x_numerical)\n",
    "        x = x_numerical\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 0.83728874\n",
      "epoch:  26 loss: 0.80933464\n",
      "epoch:  51 loss: 0.76046699\n",
      "epoch:  76 loss: 0.74447227\n",
      "epoch: 101 loss: 0.70282018\n",
      "epoch: 126 loss: 0.72070187\n",
      "epoch: 151 loss: 0.68325448\n",
      "epoch: 176 loss: 0.66910976\n",
      "epoch: 201 loss: 0.66091794\n",
      "epoch: 226 loss: 0.68803495\n",
      "epoch: 251 loss: 0.65793222\n",
      "epoch: 276 loss: 0.62715286\n",
      "epoch: 301 loss: 0.65545619\n",
      "epoch: 326 loss: 0.62976015\n",
      "epoch: 351 loss: 0.61370379\n",
      "epoch: 376 loss: 0.61023146\n",
      "epoch: 401 loss: 0.57416493\n",
      "epoch: 426 loss: 0.61317986\n",
      "epoch: 451 loss: 0.57955402\n",
      "epoch: 476 loss: 0.59583968\n",
      "epoch: 501 loss: 0.59375095\n",
      "epoch: 526 loss: 0.55978405\n",
      "epoch: 551 loss: 0.57470340\n",
      "epoch: 576 loss: 0.55338889\n",
      "epoch: 601 loss: 0.53706861\n",
      "epoch: 626 loss: 0.54669458\n",
      "epoch: 651 loss: 0.54084194\n",
      "epoch: 676 loss: 0.55004263\n",
      "epoch: 701 loss: 0.53612030\n",
      "epoch: 726 loss: 0.53327608\n",
      "epoch: 751 loss: 0.54186004\n",
      "epoch: 776 loss: 0.56177187\n",
      "epoch: 801 loss: 0.51537389\n",
      "epoch: 826 loss: 0.54188037\n",
      "epoch: 851 loss: 0.54411989\n",
      "epoch: 876 loss: 0.53557044\n",
      "epoch: 901 loss: 0.51471657\n",
      "epoch: 926 loss: 0.51528645\n",
      "epoch: 951 loss: 0.51370490\n",
      "epoch: 976 loss: 0.51892281\n",
      "epoch: 1001 loss: 0.50358301\n",
      "epoch: 1026 loss: 0.49393833\n",
      "epoch: 1051 loss: 0.49619541\n",
      "epoch: 1076 loss: 0.50894624\n",
      "epoch: 1101 loss: 0.49435407\n",
      "epoch: 1126 loss: 0.49156165\n",
      "epoch: 1151 loss: 0.49983141\n",
      "epoch: 1176 loss: 0.50222492\n",
      "epoch: 1201 loss: 0.50363034\n",
      "epoch: 1226 loss: 0.50654083\n",
      "epoch: 1251 loss: 0.50744009\n",
      "epoch: 1276 loss: 0.50172275\n",
      "epoch: 1301 loss: 0.48614377\n",
      "epoch: 1326 loss: 0.47851348\n",
      "epoch: 1351 loss: 0.48443231\n",
      "epoch: 1376 loss: 0.49759287\n",
      "epoch: 1401 loss: 0.49335420\n",
      "epoch: 1426 loss: 0.51879239\n",
      "epoch: 1451 loss: 0.48293939\n",
      "epoch: 1476 loss: 0.48516396\n",
      "epoch: 1501 loss: 0.48837999\n",
      "epoch: 1526 loss: 0.50272667\n",
      "epoch: 1551 loss: 0.47763118\n",
      "epoch: 1576 loss: 0.49089953\n",
      "epoch: 1601 loss: 0.49186710\n",
      "epoch: 1626 loss: 0.47565186\n",
      "epoch: 1651 loss: 0.50216192\n",
      "epoch: 1676 loss: 0.47237837\n",
      "epoch: 1701 loss: 0.49337295\n",
      "epoch: 1726 loss: 0.49228573\n",
      "epoch: 1751 loss: 0.48105326\n",
      "epoch: 1776 loss: 0.48874795\n",
      "epoch: 1801 loss: 0.49455425\n",
      "epoch: 1826 loss: 0.46830723\n",
      "epoch: 1851 loss: 0.46488574\n",
      "epoch: 1876 loss: 0.47941926\n",
      "epoch: 1901 loss: 0.46895167\n",
      "epoch: 1926 loss: 0.46422857\n",
      "epoch: 1951 loss: 0.49036127\n",
      "epoch: 1976 loss: 0.47875708\n",
      "epoch: 2001 loss: 0.46780482\n",
      "epoch: 2026 loss: 0.49253324\n",
      "epoch: 2051 loss: 0.46599668\n",
      "epoch: 2076 loss: 0.46665663\n",
      "epoch: 2101 loss: 0.46911278\n",
      "epoch: 2126 loss: 0.45818409\n",
      "epoch: 2151 loss: 0.47513270\n",
      "epoch: 2176 loss: 0.45778698\n",
      "epoch: 2201 loss: 0.47560641\n",
      "epoch: 2226 loss: 0.49137640\n",
      "epoch: 2251 loss: 0.44482303\n",
      "epoch: 2276 loss: 0.44121748\n",
      "epoch: 2301 loss: 0.46482170\n",
      "epoch: 2326 loss: 0.46754006\n",
      "epoch: 2351 loss: 0.45015404\n",
      "epoch: 2376 loss: 0.46360448\n",
      "epoch: 2401 loss: 0.49489748\n",
      "epoch: 2426 loss: 0.47963402\n",
      "epoch: 2451 loss: 0.46798718\n",
      "epoch: 2476 loss: 0.49743539\n",
      "epoch: 2501 loss: 0.47626662\n",
      "epoch: 2526 loss: 0.45187727\n",
      "epoch: 2551 loss: 0.46301749\n",
      "epoch: 2576 loss: 0.44987667\n",
      "epoch: 2601 loss: 0.46383619\n",
      "epoch: 2626 loss: 0.46549493\n",
      "epoch: 2651 loss: 0.43550730\n",
      "epoch: 2676 loss: 0.47083405\n",
      "epoch: 2701 loss: 0.43386766\n",
      "epoch: 2726 loss: 0.44509050\n",
      "epoch: 2751 loss: 0.47433168\n",
      "epoch: 2776 loss: 0.46097580\n",
      "epoch: 2801 loss: 0.45710313\n",
      "epoch: 2826 loss: 0.42718372\n",
      "epoch: 2851 loss: 0.43888280\n",
      "epoch: 2876 loss: 0.46321630\n",
      "epoch: 2901 loss: 0.44280678\n",
      "epoch: 2926 loss: 0.45698607\n",
      "epoch: 2951 loss: 0.45839009\n",
      "epoch: 2976 loss: 0.45064455\n",
      "epoch: 3001 loss: 0.43163446\n",
      "epoch: 3026 loss: 0.45039451\n",
      "epoch: 3051 loss: 0.44007236\n",
      "epoch: 3076 loss: 0.43801513\n",
      "epoch: 3101 loss: 0.42729363\n",
      "epoch: 3126 loss: 0.46287641\n",
      "epoch: 3151 loss: 0.42520177\n",
      "epoch: 3176 loss: 0.45163468\n",
      "epoch: 3201 loss: 0.44805586\n",
      "epoch: 3226 loss: 0.45953342\n",
      "epoch: 3251 loss: 0.42904449\n",
      "epoch: 3276 loss: 0.44097984\n",
      "epoch: 3301 loss: 0.48426649\n",
      "epoch: 3326 loss: 0.42525774\n",
      "epoch: 3351 loss: 0.42689702\n",
      "epoch: 3376 loss: 0.43560657\n",
      "epoch: 3401 loss: 0.43326101\n",
      "epoch: 3426 loss: 0.45901024\n",
      "epoch: 3451 loss: 0.44537547\n",
      "epoch: 3476 loss: 0.45710534\n",
      "epoch: 3501 loss: 0.43167964\n",
      "epoch: 3526 loss: 0.45597580\n",
      "epoch: 3551 loss: 0.41736984\n",
      "epoch: 3576 loss: 0.43687144\n",
      "epoch: 3601 loss: 0.44037998\n",
      "epoch: 3626 loss: 0.43639520\n",
      "epoch: 3651 loss: 0.43178830\n",
      "epoch: 3676 loss: 0.45348465\n",
      "epoch: 3701 loss: 0.42235470\n",
      "epoch: 3726 loss: 0.41442493\n",
      "epoch: 3751 loss: 0.42823553\n",
      "epoch: 3776 loss: 0.44116899\n",
      "epoch: 3801 loss: 0.42019200\n",
      "epoch: 3826 loss: 0.42372301\n",
      "epoch: 3851 loss: 0.43128881\n",
      "epoch: 3876 loss: 0.41817662\n",
      "epoch: 3901 loss: 0.43269736\n",
      "epoch: 3926 loss: 0.43730369\n",
      "epoch: 3951 loss: 0.44130376\n",
      "epoch: 3976 loss: 0.40929800\n",
      "epoch: 4001 loss: 0.41657683\n",
      "epoch: 4026 loss: 0.40253323\n",
      "epoch: 4051 loss: 0.42402431\n",
      "epoch: 4076 loss: 0.42816168\n",
      "epoch: 4101 loss: 0.41108105\n",
      "epoch: 4126 loss: 0.41849393\n",
      "epoch: 4151 loss: 0.42153504\n",
      "epoch: 4176 loss: 0.41679174\n",
      "epoch: 4201 loss: 0.43467593\n",
      "epoch: 4226 loss: 0.43399060\n",
      "epoch: 4251 loss: 0.41364512\n",
      "epoch: 4276 loss: 0.41195971\n",
      "epoch: 4301 loss: 0.41772628\n",
      "epoch: 4326 loss: 0.40645453\n",
      "epoch: 4351 loss: 0.43466866\n",
      "epoch: 4376 loss: 0.39747608\n",
      "epoch: 4401 loss: 0.43159321\n",
      "epoch: 4426 loss: 0.44078395\n",
      "epoch: 4451 loss: 0.42517978\n",
      "epoch: 4476 loss: 0.40650910\n",
      "epoch: 4501 loss: 0.42685357\n",
      "epoch: 4526 loss: 0.41088027\n",
      "epoch: 4551 loss: 0.42298731\n",
      "epoch: 4576 loss: 0.43695673\n",
      "epoch: 4601 loss: 0.41302058\n",
      "epoch: 4626 loss: 0.42723611\n",
      "epoch: 4651 loss: 0.42228001\n",
      "epoch: 4676 loss: 0.40246725\n",
      "epoch: 4701 loss: 0.40667135\n",
      "epoch: 4726 loss: 0.40122005\n",
      "epoch: 4751 loss: 0.39951062\n",
      "epoch: 4776 loss: 0.40716729\n",
      "epoch: 4801 loss: 0.39706331\n",
      "epoch: 4826 loss: 0.41803297\n",
      "epoch: 4851 loss: 0.40910819\n",
      "epoch: 4876 loss: 0.39946079\n",
      "epoch: 4901 loss: 0.41905144\n",
      "epoch: 4926 loss: 0.39687946\n",
      "epoch: 4951 loss: 0.41029301\n",
      "epoch: 4976 loss: 0.40807265\n",
      "epoch: 5001 loss: 0.41660315\n",
      "epoch: 5026 loss: 0.40148804\n",
      "epoch: 5051 loss: 0.41219482\n",
      "epoch: 5076 loss: 0.41018254\n",
      "epoch: 5101 loss: 0.41696471\n",
      "epoch: 5126 loss: 0.39187142\n",
      "epoch: 5151 loss: 0.41767356\n",
      "epoch: 5176 loss: 0.42640507\n",
      "epoch: 5201 loss: 0.40554664\n",
      "epoch: 5226 loss: 0.40443093\n",
      "epoch: 5251 loss: 0.39635524\n",
      "epoch: 5276 loss: 0.40904045\n",
      "epoch: 5301 loss: 0.42031130\n",
      "epoch: 5326 loss: 0.40502036\n",
      "epoch: 5351 loss: 0.38861907\n",
      "epoch: 5376 loss: 0.40895563\n",
      "epoch: 5401 loss: 0.38087669\n",
      "epoch: 5426 loss: 0.40511873\n",
      "epoch: 5451 loss: 0.38293365\n",
      "epoch: 5476 loss: 0.41340110\n",
      "epoch: 5501 loss: 0.39405093\n",
      "epoch: 5526 loss: 0.38856089\n",
      "epoch: 5551 loss: 0.37024295\n",
      "epoch: 5576 loss: 0.39568356\n",
      "epoch: 5601 loss: 0.39502266\n",
      "epoch: 5626 loss: 0.40662333\n",
      "epoch: 5651 loss: 0.37299383\n",
      "epoch: 5676 loss: 0.39542672\n",
      "epoch: 5701 loss: 0.38007891\n",
      "epoch: 5726 loss: 0.39179030\n",
      "epoch: 5751 loss: 0.37420076\n",
      "epoch: 5776 loss: 0.38733706\n",
      "epoch: 5801 loss: 0.38194880\n",
      "epoch: 5826 loss: 0.37972531\n",
      "epoch: 5851 loss: 0.37934953\n",
      "epoch: 5876 loss: 0.42037117\n",
      "epoch: 5901 loss: 0.37464145\n",
      "epoch: 5926 loss: 0.38127074\n",
      "epoch: 5951 loss: 0.38245434\n",
      "epoch: 5976 loss: 0.38454032\n",
      "epoch: 6001 loss: 0.39119869\n",
      "epoch: 6026 loss: 0.36794731\n",
      "epoch: 6051 loss: 0.38486287\n",
      "epoch: 6076 loss: 0.39052293\n",
      "epoch: 6101 loss: 0.41085759\n",
      "epoch: 6126 loss: 0.37695092\n",
      "epoch: 6151 loss: 0.37015882\n",
      "epoch: 6176 loss: 0.35836658\n",
      "epoch: 6201 loss: 0.37221059\n",
      "epoch: 6226 loss: 0.39786032\n",
      "epoch: 6251 loss: 0.36105067\n",
      "epoch: 6276 loss: 0.38226894\n",
      "epoch: 6301 loss: 0.36954936\n",
      "epoch: 6326 loss: 0.37554389\n",
      "epoch: 6351 loss: 0.40623805\n",
      "epoch: 6376 loss: 0.36154020\n",
      "epoch: 6401 loss: 0.39120826\n",
      "epoch: 6426 loss: 0.35019106\n",
      "epoch: 6451 loss: 0.38227823\n",
      "epoch: 6476 loss: 0.40282568\n",
      "epoch: 6501 loss: 0.37026858\n",
      "epoch: 6526 loss: 0.37883794\n",
      "epoch: 6551 loss: 0.38100386\n",
      "epoch: 6576 loss: 0.34751922\n",
      "epoch: 6601 loss: 0.37532386\n",
      "epoch: 6626 loss: 0.36631730\n",
      "epoch: 6651 loss: 0.37811735\n",
      "epoch: 6676 loss: 0.37443519\n",
      "epoch: 6701 loss: 0.34891751\n",
      "epoch: 6726 loss: 0.36069554\n",
      "epoch: 6751 loss: 0.40037349\n",
      "epoch: 6776 loss: 0.37102658\n",
      "epoch: 6801 loss: 0.39079934\n",
      "epoch: 6826 loss: 0.36427945\n",
      "epoch: 6851 loss: 0.33900881\n",
      "epoch: 6876 loss: 0.40079865\n",
      "epoch: 6901 loss: 0.39215127\n",
      "epoch: 6926 loss: 0.37234205\n",
      "epoch: 6951 loss: 0.37043554\n",
      "epoch: 6976 loss: 0.34852323\n",
      "epoch: 7001 loss: 0.38858411\n",
      "epoch: 7026 loss: 0.36966053\n",
      "epoch: 7051 loss: 0.37466031\n",
      "epoch: 7076 loss: 0.35381958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7101 loss: 0.40378186\n",
      "epoch: 7126 loss: 0.37979227\n",
      "epoch: 7151 loss: 0.37994522\n",
      "epoch: 7176 loss: 0.34627065\n",
      "epoch: 7201 loss: 0.35894510\n",
      "epoch: 7226 loss: 0.36814228\n",
      "epoch: 7251 loss: 0.36190918\n",
      "epoch: 7276 loss: 0.35095519\n",
      "epoch: 7301 loss: 0.36552626\n",
      "epoch: 7326 loss: 0.35380980\n",
      "epoch: 7351 loss: 0.32762632\n",
      "epoch: 7376 loss: 0.37154913\n",
      "epoch: 7401 loss: 0.35305062\n",
      "epoch: 7426 loss: 0.37468684\n",
      "epoch: 7451 loss: 0.34650490\n",
      "epoch: 7476 loss: 0.34297821\n",
      "epoch: 7501 loss: 0.34435651\n",
      "epoch: 7526 loss: 0.36323735\n",
      "epoch: 7551 loss: 0.35208139\n",
      "epoch: 7576 loss: 0.35182938\n",
      "epoch: 7601 loss: 0.35566345\n",
      "epoch: 7626 loss: 0.34667969\n",
      "epoch: 7651 loss: 0.34020078\n",
      "epoch: 7676 loss: 0.35526767\n",
      "epoch: 7701 loss: 0.33768469\n",
      "epoch: 7726 loss: 0.33710149\n",
      "epoch: 7751 loss: 0.33660725\n",
      "epoch: 7776 loss: 0.36225760\n",
      "epoch: 7801 loss: 0.33206174\n",
      "epoch: 7826 loss: 0.34985593\n",
      "epoch: 7851 loss: 0.35921258\n",
      "epoch: 7876 loss: 0.33079192\n",
      "epoch: 7901 loss: 0.35064408\n",
      "epoch: 7926 loss: 0.35381517\n",
      "epoch: 7951 loss: 0.36436406\n",
      "epoch: 7976 loss: 0.34643048\n",
      "epoch: 8001 loss: 0.33348727\n",
      "epoch: 8026 loss: 0.33558342\n",
      "epoch: 8051 loss: 0.31497642\n",
      "epoch: 8076 loss: 0.34782737\n",
      "epoch: 8101 loss: 0.30665341\n",
      "epoch: 8126 loss: 0.34350592\n",
      "epoch: 8151 loss: 0.34251842\n",
      "epoch: 8176 loss: 0.34328300\n",
      "epoch: 8201 loss: 0.34294051\n",
      "epoch: 8226 loss: 0.35712656\n",
      "epoch: 8251 loss: 0.33448327\n",
      "epoch: 8276 loss: 0.32151833\n",
      "epoch: 8301 loss: 0.31598809\n",
      "epoch: 8326 loss: 0.32632574\n",
      "epoch: 8351 loss: 0.29868281\n",
      "epoch: 8376 loss: 0.32899907\n",
      "epoch: 8401 loss: 0.34307772\n",
      "epoch: 8426 loss: 0.31275284\n",
      "epoch: 8451 loss: 0.29971793\n",
      "epoch: 8476 loss: 0.34961864\n",
      "epoch: 8501 loss: 0.33820173\n",
      "epoch: 8526 loss: 0.32864255\n",
      "epoch: 8551 loss: 0.32493109\n",
      "epoch: 8576 loss: 0.32285938\n",
      "epoch: 8601 loss: 0.31767458\n",
      "epoch: 8626 loss: 0.35480815\n",
      "epoch: 8651 loss: 0.32885796\n",
      "epoch: 8676 loss: 0.35535952\n",
      "epoch: 8701 loss: 0.34335124\n",
      "epoch: 8726 loss: 0.32491583\n",
      "epoch: 8751 loss: 0.30892327\n",
      "epoch: 8776 loss: 0.31530276\n",
      "epoch: 8801 loss: 0.30317792\n",
      "epoch: 8826 loss: 0.32858589\n",
      "epoch: 8851 loss: 0.35834089\n",
      "epoch: 8876 loss: 0.32215664\n",
      "epoch: 8901 loss: 0.34560329\n",
      "epoch: 8926 loss: 0.31772390\n",
      "epoch: 8951 loss: 0.32817659\n",
      "epoch: 8976 loss: 0.30762532\n",
      "epoch: 9001 loss: 0.32242936\n",
      "epoch: 9026 loss: 0.29006302\n",
      "epoch: 9051 loss: 0.31018138\n",
      "epoch: 9076 loss: 0.28699186\n",
      "epoch: 9101 loss: 0.31267971\n",
      "epoch: 9126 loss: 0.32198611\n",
      "epoch: 9151 loss: 0.33148155\n",
      "epoch: 9176 loss: 0.28862187\n",
      "epoch: 9201 loss: 0.33287126\n",
      "epoch: 9226 loss: 0.35479411\n",
      "epoch: 9251 loss: 0.30187246\n",
      "epoch: 9276 loss: 0.30639508\n",
      "epoch: 9301 loss: 0.30873418\n",
      "epoch: 9326 loss: 0.31274411\n",
      "epoch: 9351 loss: 0.32115486\n",
      "epoch: 9376 loss: 0.28723687\n",
      "epoch: 9401 loss: 0.33969995\n",
      "epoch: 9426 loss: 0.31314349\n",
      "epoch: 9451 loss: 0.31991860\n",
      "epoch: 9476 loss: 0.33519757\n",
      "epoch: 9501 loss: 0.30784079\n",
      "epoch: 9526 loss: 0.38390487\n",
      "epoch: 9551 loss: 0.30910200\n",
      "epoch: 9576 loss: 0.28305462\n",
      "epoch: 9601 loss: 0.30780926\n",
      "epoch: 9626 loss: 0.27955860\n",
      "epoch: 9651 loss: 0.31519386\n",
      "epoch: 9676 loss: 0.30787528\n",
      "epoch: 9701 loss: 0.32546598\n",
      "epoch: 9726 loss: 0.29397249\n",
      "epoch: 9751 loss: 0.33783609\n",
      "epoch: 9776 loss: 0.31975269\n",
      "epoch: 9801 loss: 0.29129666\n",
      "epoch: 9826 loss: 0.31733048\n",
      "epoch: 9851 loss: 0.29635563\n",
      "epoch: 9876 loss: 0.30721033\n",
      "epoch: 9901 loss: 0.28144380\n",
      "epoch: 9926 loss: 0.32899767\n",
      "epoch: 9951 loss: 0.31827903\n",
      "epoch: 9976 loss: 0.28030783\n",
      "epoch: 10001 loss: 0.28260779\n",
      "epoch: 10026 loss: 0.30925301\n",
      "epoch: 10051 loss: 0.29302284\n",
      "epoch: 10076 loss: 0.28151679\n",
      "epoch: 10101 loss: 0.33129880\n",
      "epoch: 10126 loss: 0.27454320\n",
      "epoch: 10151 loss: 0.29029939\n",
      "epoch: 10176 loss: 0.29571325\n",
      "epoch: 10201 loss: 0.33255154\n",
      "epoch: 10226 loss: 0.31385794\n",
      "epoch: 10251 loss: 0.29929727\n",
      "epoch: 10276 loss: 0.31567222\n",
      "epoch: 10301 loss: 0.34377754\n",
      "epoch: 10326 loss: 0.31559974\n",
      "epoch: 10351 loss: 0.30922619\n",
      "epoch: 10376 loss: 0.28429991\n",
      "epoch: 10401 loss: 0.29551014\n",
      "epoch: 10426 loss: 0.28127348\n",
      "epoch: 10451 loss: 0.32598490\n",
      "epoch: 10476 loss: 0.34927160\n",
      "epoch: 10501 loss: 0.31330547\n",
      "epoch: 10526 loss: 0.28207475\n",
      "epoch: 10551 loss: 0.31840453\n",
      "epoch: 10576 loss: 0.29800335\n",
      "epoch: 10601 loss: 0.29043838\n",
      "epoch: 10626 loss: 0.32967657\n",
      "epoch: 10651 loss: 0.28746754\n",
      "epoch: 10676 loss: 0.27988893\n",
      "epoch: 10701 loss: 0.27388647\n",
      "epoch: 10726 loss: 0.33029312\n",
      "epoch: 10751 loss: 0.29388112\n",
      "epoch: 10776 loss: 0.29104906\n",
      "epoch: 10801 loss: 0.26746213\n",
      "epoch: 10826 loss: 0.30965582\n",
      "epoch: 10851 loss: 0.31586176\n",
      "epoch: 10876 loss: 0.30337235\n",
      "epoch: 10901 loss: 0.30630803\n",
      "epoch: 10926 loss: 0.26279780\n",
      "epoch: 10951 loss: 0.31451666\n",
      "epoch: 10976 loss: 0.29961619\n",
      "epoch: 11001 loss: 0.27811748\n",
      "epoch: 11026 loss: 0.33214039\n",
      "epoch: 11051 loss: 0.29068646\n",
      "epoch: 11076 loss: 0.29367748\n",
      "epoch: 11101 loss: 0.27803469\n",
      "epoch: 11126 loss: 0.28707495\n",
      "epoch: 11151 loss: 0.28431588\n",
      "epoch: 11176 loss: 0.26993519\n",
      "epoch: 11201 loss: 0.28085154\n",
      "epoch: 11226 loss: 0.30980575\n",
      "epoch: 11251 loss: 0.27812028\n",
      "epoch: 11276 loss: 0.27479911\n",
      "epoch: 11301 loss: 0.29105213\n",
      "epoch: 11326 loss: 0.27421638\n",
      "epoch: 11351 loss: 0.27986056\n",
      "epoch: 11376 loss: 0.30096570\n",
      "epoch: 11401 loss: 0.30818436\n",
      "epoch: 11426 loss: 0.27525511\n",
      "epoch: 11451 loss: 0.27015224\n",
      "epoch: 11476 loss: 0.27421540\n",
      "epoch: 11501 loss: 0.30388746\n",
      "epoch: 11526 loss: 0.31517935\n",
      "epoch: 11551 loss: 0.29519606\n",
      "epoch: 11576 loss: 0.25911441\n",
      "epoch: 11601 loss: 0.27845472\n",
      "epoch: 11626 loss: 0.29778624\n",
      "epoch: 11651 loss: 0.27514306\n",
      "epoch: 11676 loss: 0.26337898\n",
      "epoch: 11701 loss: 0.29141328\n",
      "epoch: 11726 loss: 0.26620933\n",
      "epoch: 11751 loss: 0.28255796\n",
      "epoch: 11776 loss: 0.26906741\n",
      "epoch: 11801 loss: 0.29819000\n",
      "epoch: 11826 loss: 0.30182129\n",
      "epoch: 11851 loss: 0.28504607\n",
      "epoch: 11876 loss: 0.28384387\n",
      "epoch: 11901 loss: 0.28135535\n",
      "epoch: 11926 loss: 0.29361773\n",
      "epoch: 11951 loss: 0.29133645\n",
      "epoch: 11976 loss: 0.24196641\n",
      "epoch: 12000 loss: 0.2509675622\n"
     ]
    }
   ],
   "source": [
    "model = Model(numerical_data.shape[1], 2, [150,75,50,25,25,25], p=0.4)\n",
    "\n",
    "#loss_function = nn.CrossEntropyLoss(weight = torch.Tensor([1.0, 1.1]))\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#loss_function = nn.NLLLoss()\n",
    "#loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=5e-4, momentum=.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 12000\n",
    "aggregated_losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(numerical_train_data)\n",
    "    single_loss = loss_function(y_pred, train_outputs)\n",
    "    aggregated_losses.append(single_loss)\n",
    "\n",
    "    if i%25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.22909474\n"
     ]
    }
   ],
   "source": [
    "#prints out validation loss\n",
    "with torch.no_grad():\n",
    "    validation_y_val = model(numerical_test_data)\n",
    "    loss = loss_function(validation_y_val, test_outputs)\n",
    "print(f'Loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.27456668\n"
     ]
    }
   ],
   "source": [
    "#prints out training loss\n",
    "with torch.no_grad():\n",
    "    train_y_val = model(numerical_train_data)\n",
    "    loss = loss_function(train_y_val, train_outputs)\n",
    "print(f'Loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.6896551724137931 - F1 Score =  0.6905576953757093\n",
      "Train Accuracy:  0.8672032193158954 - F1 Score =  0.8676027755762155\n"
     ]
    }
   ],
   "source": [
    "validation_y_output = np.argmax(validation_y_val, axis=1)\n",
    "train_y_output = np.argmax(train_y_val, axis=1) \n",
    "\n",
    "validation_accuracy = np.mean(validation_y_output.numpy() == test_outputs.numpy())\n",
    "validation_f1 = f1_score(validation_y_output.numpy(), test_outputs.numpy(), average='weighted')\n",
    "\n",
    "train_accuracy = np.mean(train_y_output.numpy() == train_outputs.numpy())\n",
    "train_f1 = f1_score(train_y_output.numpy(), train_outputs.numpy(), average='weighted')\n",
    "\n",
    "print(\"Validation Accuracy: \", validation_accuracy, \"- F1 Score = \", validation_f1)\n",
    "print(\"Train Accuracy: \", train_accuracy, \"- F1 Score = \", train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set (F1 Score Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = read_data(\"../data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = add_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_numerical_data = np.stack([test_dataset[col].values for col in numerical_columns], 1)\n",
    "testset_numerical_data = torch.tensor(testset_numerical_data, dtype=torch.float)\n",
    "\n",
    "testset_outputs = torch.tensor(test_dataset[output_label].values).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.66373986\n"
     ]
    }
   ],
   "source": [
    "#prints out testing loss\n",
    "with torch.no_grad():\n",
    "    y_val = model(testset_numerical_data)\n",
    "    loss = loss_function(y_val, testset_outputs)\n",
    "print(f'Loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.7988505747126436 - F1 Score =  0.8004079910773623\n"
     ]
    }
   ],
   "source": [
    "y_output = np.argmax(y_val, axis=1)\n",
    "accuracy = np.mean(y_output.numpy() == testset_outputs.numpy())\n",
    "f1 = f1_score(y_output.numpy(), testset_outputs.numpy(), average='weighted')\n",
    "print(\"Testing Accuracy: \",accuracy, \"- F1 Score = \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59 23]\n",
      " [12 80]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.72      0.77        82\n",
      "           1       0.78      0.87      0.82        92\n",
      "\n",
      "    accuracy                           0.80       174\n",
      "   macro avg       0.80      0.79      0.80       174\n",
      "weighted avg       0.80      0.80      0.80       174\n",
      "\n",
      "0.7988505747126436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(testset_outputs,y_output))\n",
    "print(classification_report(testset_outputs,y_output))\n",
    "print(accuracy_score(testset_outputs, y_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS 4780 Final Project Student Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
